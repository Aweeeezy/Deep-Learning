{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'bbox_inches':'tight'}\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from numpy.random import random, seed\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. (40 pts) Define functions\n",
    "### Define the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    \"\"\"\n",
    "        Turn single digit numerical value classes into length 10 vectors...\n",
    "        1 for the positive class and 0's for the other 9 negative classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_l, max_l = min(labels), max(labels)\n",
    "    one_hot_map = {k: [0 if i < k else 1 if i == k else 0 \\\n",
    "                              for i in range(min_l, max_l+1)] \\\n",
    "                      for k in range(min_l, max_l+1)}\n",
    "    \n",
    "    return np.matrix([one_hot_map[label[0]] for label in labels]).reshape(10, -1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "        Squishes the output of our hypothesis function into the range (0, 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1./(1. + np.exp(-z))\n",
    "\n",
    "def softmax(outputs):\n",
    "    \"\"\"\n",
    "        Produce confidence probabilities for the output classes using the output of the\n",
    "        second-from-last layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    exp = np.exp(outputs)\n",
    "    return exp/exp.sum(axis=0)\n",
    "\n",
    "    \n",
    "def log_loss(output, labels):\n",
    "    \"\"\"\n",
    "        Computes the multi-class log loss when provided with the output from the softmax\n",
    "        group and the corresponding labeled data\n",
    "    \"\"\"\n",
    "    \n",
    "    return (1/len(labels.T)) * -np.sum(np.dot(labels.T, np.log(output)))\n",
    "    \n",
    "def forward(net, X):\n",
    "    \"\"\"\n",
    "        Perform forward propagation through our neural network. Apply sigmoid to\n",
    "        the first weighted sum; apply softmax to the second weighted sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve weights from the network and reinitialize the cached activations\n",
    "    W1 = net['weights'][0]\n",
    "    W2 = net['weights'][1]\n",
    "    net['activations'] = []\n",
    "           \n",
    "    # Perform forward pass while appyling bias terms to the input and a1 layers\n",
    "    X_bias = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)   \n",
    "    a1 = sigmoid(np.dot(W1, X_bias.T))                               \n",
    "    a1_bias = np.concatenate((a1, np.ones((1, X.shape[0]))), axis=0) \n",
    "    a2 = softmax(np.dot(W2, a1_bias))                                \n",
    "    \n",
    "    # Cache input data and activations\n",
    "    net['input'] = X  \n",
    "    net['activations'].append(a1)\n",
    "    net['activations'].append(a2)\n",
    "    \n",
    "    return a2\n",
    "    \n",
    "def backward(net, labels):\n",
    "    \"\"\"\n",
    "        Perform back propagation through our neural network. Compute error derivatives\n",
    "        w.r.t. weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve input data, cached activations, and weights from the network\n",
    "    X = net['input']\n",
    "    a1 = net['activations'][0]\n",
    "    a2 = net['activations'][1]\n",
    "    W1 = net['weights'][0][:, :-1]\n",
    "    b1 = net['weights'][0][:, -1].reshape(-1, 1)\n",
    "    W2 = net['weights'][1][:, :-1]\n",
    "    b2 = net['weights'][1][:, -1].reshape(-1, 1)\n",
    "\n",
    "    # Compute loss function derivatives w.r.t. parameters and biases\n",
    "    dL_dW2 = (1/len(X)) * np.dot((a2-labels), a1.T)\n",
    "    dL_db2 = (1/len(X)) * np.sum((a2-labels), axis=1, keepdims=True)\n",
    "    dL_dW1 = (1/len(X)) * np.dot(np.dot(np.dot(W2.T, (a2-labels)), np.dot((1-a1).T, a1)), X)\n",
    "    dL_db1 = (1/len(X)) * np.sum(np.dot(np.dot(W2.T, (a2-labels)), np.dot((1-a1).T, a1)), axis=1, keepdims=True)\n",
    "    \n",
    "    return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "        \n",
    "def gradient_descent(net, X, labels, alpha):\n",
    "    \"\"\"\n",
    "        Use loss function derivatives w.r.t weights to continually improve the\n",
    "        performace of our network.\n",
    "    \"\"\"\n",
    "      \n",
    "    def update_weights(weights, gradients):\n",
    "        W1, b1 = weights[0][:, :-1], weights[0][:, -1].reshape(-1, 1)\n",
    "        W2, b2 = weights[1][:, :-1], weights[1][:, -1].reshape(-1, 1)\n",
    "\n",
    "        W1 -= alpha * gradients[0]\n",
    "        b1 -= alpha * gradients[1]\n",
    "        W2 -= alpha * gradients[2]\n",
    "        b2 -= alpha * gradients[3]  \n",
    "\n",
    "    new_cost, old_cost = log_loss(forward(net, X), labels), float('inf')    \n",
    "    beginning = start = time()\n",
    "    count = 0\n",
    "    \n",
    "    print('Initial cost:', new_cost)\n",
    "    \n",
    "    while new_cost < old_cost and abs(old_cost - new_cost) > 1e-5:\n",
    "        update_weights(net['weights'], backward(net, labels))\n",
    "        old_cost = new_cost\n",
    "        new_cost = log_loss(forward(net, X), labels)\n",
    "        if time() - start > 20:\n",
    "            print('\\t', new_cost)\n",
    "            start = time()\n",
    "        count += 1\n",
    "    \n",
    "    end = time() - beginning\n",
    "    train_eval = log_loss(forward(net, X), labels)\n",
    "        \n",
    "    print('Final model has a log loss of {} -- achieved in {} iterations in {} seconds'.format(\n",
    "            train_eval, count, end))\n",
    "    \n",
    "    return ()\n",
    "    \n",
    "def save_weights(weights):\n",
    "    \"\"\"\n",
    "        Store learned model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('nn_weights.npy', 'wb') as f:\n",
    "        np.save(f, weights)\n",
    "        \n",
    "def load_weights():\n",
    "    \"\"\"\n",
    "        If a weights file is present, load pretrained model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('nn_weights.npy', 'rb') as f:\n",
    "            return np.load(f)\n",
    "    except:\n",
    "        raise 'No weights file found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. (5 pts) Split data\n",
    "### Split training and testing data into x and y sets. Input has columns from 0 to 399 and ouput has a column 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('ex3_train.csv')\n",
    "test = pd.read_csv('ex3_test.csv')\n",
    "\n",
    "x_train = train.iloc[:, :-1].as_matrix()\n",
    "x_test = test.iloc[:, :-1].as_matrix()\n",
    "\n",
    "# Min-max scale inputs to facilitate convergence\n",
    "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
    "x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())\n",
    "\n",
    "# One hot encode labels\n",
    "y_train = one_hot_encode(train.iloc[:, -1].values.reshape(-1, 1))\n",
    "y_test = one_hot_encode(test.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. (5 pts) Initialize parameters\n",
    "### Use np.random.seed(1) when initializing weight coefficients. Set bias terms to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Form weight matrices by concatenating zeroed bias weights to randomly initialized parameter weights\n",
    "seed(1); W1 = np.concatenate((random((25, x_train.shape[1])), np.zeros((25, 1))), axis=1)\n",
    "seed(1); W2 = np.concatenate((random((y_train.shape[0], 25)), np.zeros((y_train.shape[0], 1))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. (20 pts) Neural network model with one hidden layer\n",
    "### Build a neural network model, using the training set, with an input layer of 400 neurons, one hidden layer of 25 neurons, and an output layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find precomputed weights...training model parameters from scratch...\n",
      "Initial cost: 10637.5151742\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,3500) (3500,3500) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7ca8026990ff>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn_weights.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nn_weights.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ec3a3c35246e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded precomputed weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7ca8026990ff>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0;34m'No weights file found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ec3a3c35246e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not find precomputed weights...training model parameters from scratch...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7ca8026990ff>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(net, X, labels, alpha)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnew_cost\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mold_cost\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_cost\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnew_cost\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mold_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mnew_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7ca8026990ff>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(net, labels)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdL_dW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdL_db2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdL_dW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mdL_db1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,3500) (3500,3500) "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    [X: (n,400)+bias -> W1: (25,401) -> sig: (25,n)+bias -> W2: (10,26) -> softmax: (10,n)] -> Loss: -sum(t*logp)\n",
    "\"\"\"\n",
    "\n",
    "net = {'weights': [W1, W2],\n",
    "       'activations': []\n",
    "      }\n",
    "\n",
    "try:\n",
    "    net['weights'] = load_weights()\n",
    "    print('Loaded precomputed weights')\n",
    "except:\n",
    "    print('Could not find precomputed weights...training model parameters from scratch...')\n",
    "    gradient_descent(net, x_train, y_train, alpha=0.5)\n",
    "    save_weights(net['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. (10 pts) Predictions\n",
    "### Predict digits using the softmax function. Calculate the accuracy of the predictions using both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_eval = log_loss(forward(net, x_train), y_train)\n",
    "test_eval = log_loss(forward(net, x_test), y_test)\n",
    "print('Training: {}\\nTesting: {}'.format(training_eval, testing_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. (20 pts) Optimization\n",
    "### Optimize the model using a variety of learning rates and number of iterations. Plot the cost over the number of iterations with different learning rates for the training set. Print the optimized accuracy for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for a in [1, 0.5, 0.1]:\n",
    "    gradient_descent(net, x_train, y_train, alpha=a)\n",
    "    train_eval = log_loss(forward(net, x_train), y_train)\n",
    "    test_eval = log_loss(forward(net, x_test), y_test)\n",
    "    scores.append((train_eval, test_eval, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
