{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'bbox_inches':'tight'}\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import uniform, seed\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. (40 pts) Define functions\n",
    "### Define the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    \"\"\"\n",
    "        Turn single digit numerical value classes into length 10 vectors...\n",
    "        1 for the positive class and 0's for the other 9 negative classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_l, max_l = min(labels), max(labels)\n",
    "    one_hot_map = {k: [0 if i < k else 1 if i == k else 0 \\\n",
    "                              for i in range(min_l, max_l+1)] \\\n",
    "                      for k in range(min_l, max_l+1)}\n",
    "    \n",
    "    return np.matrix([one_hot_map[label[0]] for label in labels]).reshape(-1, 10)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "        Squishes the output of our hypothesis function into the range (0, 1).\n",
    "    \"\"\"\n",
    "       \n",
    "    z = np.clip(z, -500, 500)\n",
    "    exp = np.exp(z)\n",
    "    result = exp/(exp+1)\n",
    "    return result\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Produce confidence probabilities for the output classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    exp = np.exp(z)\n",
    "    return exp/exp.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "def log_loss(output, labels):\n",
    "    \"\"\"\n",
    "        Computes the multi-class log loss when provided with the output from the softmax\n",
    "        group and the corresponding labeled data\n",
    "    \"\"\"\n",
    "\n",
    "    return (1/labels.shape[0]) * -np.sum(np.multiply(labels, np.log(output)) + np.multiply((1-labels), np.log(1-output)))\n",
    "    \n",
    "def forward(net, X):\n",
    "    \"\"\"\n",
    "        Perform forward propagation through our neural network. Apply sigmoid to\n",
    "        the first weighted sum; apply softmax to the second weighted sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve weights from the network and reinitialize the cached activations\n",
    "    W1 = net['weights'][0]\n",
    "    W2 = net['weights'][1]\n",
    "    net['activations'] = []\n",
    "           \n",
    "    # Perform forward pass while appyling bias terms to the input and a1 layers\n",
    "    X_bias = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)   \n",
    "    a1 = sigmoid(np.dot(W1, X_bias.T))                              \n",
    "    a1_bias = np.concatenate((a1, np.ones((1, X.shape[0]))), axis=0) \n",
    "    a2 = sigmoid(np.dot(W2, a1_bias)).T    \n",
    "    \n",
    "    # Cache input data and activations\n",
    "    net['input'] = X  \n",
    "    net['activations'].append(a1)\n",
    "    net['activations'].append(a2)\n",
    "                \n",
    "    return a2\n",
    "    \n",
    "def backward(net, labels):\n",
    "    \"\"\"\n",
    "        Perform back propagation through our neural network. Compute error derivatives\n",
    "        w.r.t. weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve input data, cached activations, and weights from the network\n",
    "    X = net['input']\n",
    "    a1 = net['activations'][0]\n",
    "    a2 = net['activations'][1]\n",
    "    W1 = net['weights'][0][:, :-1]\n",
    "    W2 = net['weights'][1][:, :-1]\n",
    "\n",
    "    # Compute loss function derivatives w.r.t. parameters and biases\n",
    "    dL_dW2 = (1/X.shape[0]) * np.dot(a1, (a2-labels)).T\n",
    "    dL_db2 = (1/X.shape[0]) * np.sum((a2-labels), axis=0, keepdims=True).T\n",
    "    dL_dW1 = (1/X.shape[0]) * np.dot(np.dot(np.dot((a2-labels), W2).T, np.dot(a1.T, (1-a1))), X)\n",
    "    dL_db1 = (1/X.shape[0]) * np.sum(np.dot(np.dot((a2-labels), W2).T, np.dot(a1.T, (1-a1))), axis=1, keepdims=True)\n",
    "       \n",
    "    return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "        \n",
    "def gradient_descent(net, X, labels, alpha, iters):\n",
    "    \"\"\"\n",
    "        Use loss function derivatives w.r.t weights to continually improve the\n",
    "        performace of our network.\n",
    "    \"\"\"\n",
    "      \n",
    "    def update_weights(weights, gradients):\n",
    "        W1, b1 = weights[0][:, :-1], weights[0][:, -1].reshape(-1, 1)\n",
    "        W2, b2 = weights[1][:, :-1], weights[1][:, -1].reshape(-1, 1)\n",
    "        \n",
    "        W1 -= alpha * gradients[0]\n",
    "        b1 -= alpha * gradients[1]\n",
    "        W2 -= alpha * gradients[2]\n",
    "        b2 -= alpha * gradients[3]  \n",
    "        \n",
    "    new_cost, old_cost = log_loss(forward(net, X), labels), float('inf')    \n",
    "    beginning = start = time()\n",
    "    count = 0\n",
    "    \n",
    "    print('Initial cost:', new_cost)\n",
    "    \n",
    "    while count < iters:\n",
    "        update_weights(net['weights'], backward(net, labels))\n",
    "        old_cost = new_cost\n",
    "        new_cost = log_loss(forward(net, X), labels)\n",
    "        if time() - start > 10:\n",
    "            print('\\t', new_cost)\n",
    "            start = time()\n",
    "        count += 1\n",
    "            \n",
    "    elapsed = time() - beginning\n",
    "    train_eval = old_cost\n",
    "        \n",
    "    print('Final model has a log loss of {} -- achieved in {} iterations in {} seconds'.format(\n",
    "            train_eval, iters, elapsed))\n",
    "    \n",
    "    return (train_eval, elapsed)\n",
    "    \n",
    "def save_weights(weights):\n",
    "    \"\"\"\n",
    "        Store learned model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('nn_weights.npy', 'wb') as f:\n",
    "        np.save(f, weights)\n",
    "        \n",
    "def load_weights():\n",
    "    \"\"\"\n",
    "        If a weights file is present, load pretrained model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('nn_weights.npy', 'rb') as f:\n",
    "            return np.load(f)\n",
    "    except:\n",
    "        raise 'No weights file found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. (5 pts) Split data\n",
    "### Split training and testing data into x and y sets. Input has columns from 0 to 399 and ouput has a column 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('ex3_train.csv')\n",
    "test = pd.read_csv('ex3_test.csv')\n",
    "\n",
    "x_train = train.iloc[:, :-1].as_matrix()\n",
    "x_test = test.iloc[:, :-1].as_matrix()\n",
    "\n",
    "# One hot encode labels\n",
    "y_train = one_hot_encode(train.iloc[:, -1].values.reshape(-1, 1))\n",
    "y_test = one_hot_encode(test.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. (5 pts) Initialize parameters\n",
    "### Use np.random.seed(1) when initializing weight coefficients. Set bias terms to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Form weight matrices by concatenating zeroed bias weights to randomly initialized parameter weights\n",
    "seed(1); W1_init = np.concatenate((uniform(-1, 1, (25, x_train.shape[1])), np.zeros((25, 1))), axis=1)\n",
    "seed(1); W2_init = np.concatenate((uniform(-1, 1, (y_train.shape[1], 25)), np.zeros((y_train.shape[1], 1))), axis=1)\n",
    "W1 = np.copy(W1_init)\n",
    "W2 = np.copy(W2_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. (20 pts) Neural network model with one hidden layer\n",
    "### Build a neural network model, using the training set, with an input layer of 400 neurons, one hidden layer of 25 neurons, and an output layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find precomputed weights...training model parameters from scratch...\n",
      "Initial cost: 10.86632526\n",
      "\t 3.27168839759\n",
      "\t 3.2508535804\n",
      "\t 3.25068554154\n",
      "\t 3.25068487476\n",
      "Final model has a log loss of 3.250684875383155 -- achieved in 100 iterations in 40.63325762748718 seconds\n"
     ]
    }
   ],
   "source": [
    "net = {'weights': [W1, W2],\n",
    "       'activations': []\n",
    "      }\n",
    "\n",
    "try:\n",
    "    net['weights'] = load_weights()\n",
    "    print('Loaded precomputed weights')\n",
    "except:\n",
    "    print('Could not find precomputed weights...training model parameters from scratch...')\n",
    "    gradient_descent(net, x_train, y_train, alpha=0.1, iters=100)\n",
    "    #save_weights(net['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. (10 pts) Predictions\n",
    "### Predict digits using the softmax function. Calculate the accuracy of the predictions using both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_prob = softmax(forward(net, x_train))\n",
    "test_prob = softmax(forward(net, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. (20 pts) Optimization\n",
    "### Optimize the model using a variety of learning rates and number of iterations. Plot the cost over the number of iterations with different learning rates for the training set. Print the optimized accuracy for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "learning_rates = [10, 1, 0.5, 0.1, 0.05, 0.01]\n",
    "num_iters = [5, 10, 25]\n",
    "for a in learning_rates:\n",
    "    for i in num_iters:\n",
    "        net['weights'][0], net['weights'][1] = np.copy(W1_init), np.copy(W2_init)\n",
    "        train_eval, elapsed = gradient_descent(net, x_train, y_train, alpha=a, iters=i)\n",
    "        test_eval = log_loss(forward(net, x_test), y_test)\n",
    "        scores.append((train_eval, test_eval, elapsed, a, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
