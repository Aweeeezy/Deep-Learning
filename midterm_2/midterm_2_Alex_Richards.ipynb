{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for loading/augmenting data and building/training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "\n",
    "def load_data():\n",
    "    x_train = np.load('exam2_train_x.npy').astype('float32')\n",
    "    y_train = np.load('exam2_train_y.npy').astype('float32')\n",
    "    \"\"\"\n",
    "    if os.path.exists('x_train_augmented.npy') and os.path.exists('y_train_augmented.npy'):\n",
    "        print \"Training data has been augmented by rotating and flipping horizontally\"\n",
    "        x_train = np.load('x_train_augmented.npy').astype('float32')\n",
    "        y_train = np.load('y_train_augmented.npy').astype('float32')\n",
    "    else:\n",
    "        print \"Training data is not yet augmented...\"\n",
    "        x_train = np.load('exam2_train_x.npy').astype('float32')\n",
    "        y_train = np.load('exam2_train_y.npy').astype('float32')\n",
    "    \"\"\"  \n",
    "       \n",
    "    x_test = np.load('exam2_test_x.npy').astype('float32')\n",
    "    y_test = np.load('exam2_test_y.npy').astype('float32')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def rotate_imgs(x, y, d=20):\n",
    "    \"\"\"\n",
    "        Makes a copy of X and Y -- image and label arrays.\n",
    "        Rotates each image D degrees left and right.\n",
    "        Appends these rotated images and their corresponding labels to the copied arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_x, new_y = np.copy(x), np.copy(y)\n",
    "    for image, label in zip(x, y):\n",
    "        \n",
    "        image_l = transform.rotate(image, d, mode='edge', resize=False, preserve_range=True)\n",
    "        image_r = transform.rotate(image, -d, mode='edge', resize=False, preserve_range=True)\n",
    "        \n",
    "        new_x = np.append(new_x, [image_l], axis=0)\n",
    "        new_y = np.append(new_y, [label], axis=0)\n",
    "        new_x = np.append(new_x, [image_r], axis=0)\n",
    "        new_y = np.append(new_y, [label], axis=0)\n",
    "                \n",
    "    return new_x.astype('float32'), new_y\n",
    "\n",
    "def flip_imgs(x, y):\n",
    "    \"\"\"\n",
    "        Makes a copy of X and Y -- image and label arrays.\n",
    "        Flips each image horizontally.\n",
    "        Appends these flipped images and their corresponding labels to the copied arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_x, new_y = np.copy(x), np.copy(y)\n",
    "    for image, label in zip(x, y):\n",
    "\n",
    "        new_x = np.append(new_x, [np.fliplr(image)], axis=0)\n",
    "        new_y = np.append(new_y, [label], axis=0)\n",
    "    \n",
    "    return new_x.astype('float32'), new_y\n",
    "\n",
    "def conv_model(features, labels, mode): \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 3])\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[4, 4],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=128,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    pool3_flat = tf.reshape(pool3, [-1, 8 * 8 * 128])\n",
    "    \n",
    "    dense1 = tf.layers.dense(inputs=pool3_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout1 = tf.layers.dropout(inputs=dense1, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=512, activation=tf.nn.relu)\n",
    "    dropout2 = tf.layers.dropout(inputs=dense2, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=dropout2, units=NUM_CLASSES, activation=tf.nn.sigmoid)\n",
    "        \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=NUM_CLASSES)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "  \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data and, if necessary, augment it by rotating and flipping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif x_train.shape[0] == 1020 or y_train.shape[0] == 1020:\\n    print \"\\tLet\\'s rotate and flip the training data images to produce an augmented training set.\"\\n    \\n    x_train, y_train = rotate_imgs(x_train, y_train)\\n    x_train, y_train = flip_imgs(x_train, y_train)\\n    \\n    print \"\\tAfter rotating and flipping the images of the training set, we have {} images and {} labels.\".format(\\n        x_train.shape[0], y_train.shape[0])\\n    \\n    np.save(\\'x_train_augmented.npy\\', x_train)\\n    np.save(\\'y_train_augmented.npy\\', y_train)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "\"\"\"\n",
    "if x_train.shape[0] == 1020 or y_train.shape[0] == 1020:\n",
    "    print \"\\tLet's rotate and flip the training data images to produce an augmented training set.\"\n",
    "    \n",
    "    x_train, y_train = rotate_imgs(x_train, y_train)\n",
    "    x_train, y_train = flip_imgs(x_train, y_train)\n",
    "    \n",
    "    print \"\\tAfter rotating and flipping the images of the training set, we have {} images and {} labels.\".format(\n",
    "        x_train.shape[0], y_train.shape[0])\n",
    "    \n",
    "    np.save('x_train_augmented.npy', x_train)\n",
    "    np.save('y_train_augmented.npy', y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': 'convnet_model', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into convnet_model/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.24384595 0.08970606 0.08971474 0.24384595 0.24318142 0.08970591]\n",
      " [0.11395329 0.23719078 0.23714922 0.0872577  0.087258   0.237191  ]\n",
      " [0.07843921 0.20878366 0.07785791 0.21163975 0.21163975 0.21163975]\n",
      " [0.1862933  0.1862933  0.1862933  0.1862933  0.06853348 0.1862933 ]\n",
      " [0.21115923 0.07768122 0.21115945 0.21115945 0.07768124 0.21115945]\n",
      " [0.06853348 0.1862933  0.1862933  0.1862933  0.1862933  0.1862933 ]\n",
      " [0.18672445 0.06869209 0.18672445 0.18672445 0.18672445 0.18441014]\n",
      " [0.20764174 0.09304559 0.20764174 0.20764174 0.07638757 0.20764169]\n",
      " [0.1666667  0.16666648 0.1666667  0.1666667  0.1666667  0.1666667 ]\n",
      " [0.16856185 0.16726336 0.16856185 0.15848924 0.16856185 0.16856183]\n",
      " [0.16666709 0.16666596 0.16666709 0.16666709 0.16666709 0.1666657 ]\n",
      " [0.2436861  0.08964711 0.08964757 0.2436861  0.2436861  0.08964711]\n",
      " [0.20076054 0.12310219 0.20076054 0.07385568 0.20076054 0.20076054]\n",
      " [0.11174193 0.24917242 0.30374604 0.11185579 0.11174193 0.11174193]\n",
      " [0.18631239 0.18631239 0.18631239 0.18631239 0.18620992 0.0685405 ]\n",
      " [0.21099868 0.21099868 0.21099868 0.21099868 0.07838322 0.07762208]\n",
      " [0.2436861  0.08964753 0.08964711 0.2436861  0.08964711 0.2436861 ]\n",
      " [0.08964749 0.24368714 0.24368714 0.24368334 0.08964749 0.08964749]\n",
      " [0.21115941 0.07768121 0.21115941 0.21115941 0.07768121 0.21115941]\n",
      " [0.17080988 0.17080988 0.17080988 0.17080982 0.14595063 0.17080988]\n",
      " [0.18700086 0.18320277 0.18700086 0.18700086 0.18700086 0.06879377]\n",
      " [0.10782046 0.27636576 0.10778838 0.2926794  0.10767314 0.10767285]\n",
      " [0.08964565 0.08964564 0.2436821  0.08966242 0.2436821  0.2436821 ]\n",
      " [0.22503608 0.09566513 0.24831915 0.09136036 0.09135151 0.24826771]\n",
      " [0.10597079 0.10597079 0.28805846 0.28805846 0.10597079 0.10597079]\n",
      " [0.07421438 0.20173262 0.2017347  0.20173553 0.20173553 0.11884721]\n",
      " [0.0896498  0.0896498  0.24369343 0.24366377 0.0896498  0.24369343]\n",
      " [0.08964714 0.08964714 0.24368617 0.08964714 0.2436862  0.2436862 ]\n",
      " [0.07768846 0.0776806  0.21115774 0.21115774 0.21115774 0.21115774]\n",
      " [0.18582861 0.18582861 0.18582861 0.18582861 0.18582861 0.07085696]\n",
      " [0.21115941 0.07768121 0.21115941 0.21115941 0.21115941 0.07768121]\n",
      " [0.1862933  0.1862933  0.1862933  0.06853348 0.1862933  0.1862933 ]\n",
      " [0.10584685 0.10701624 0.28772157 0.28772157 0.10584685 0.10584685]\n",
      " [0.2436862  0.08964714 0.2436862  0.08964714 0.2436862  0.08964714]\n",
      " [0.18629488 0.1862904  0.18629488 0.18629488 0.18629093 0.06853406]\n",
      " [0.08964589 0.2436813  0.08966479 0.08964534 0.2436813  0.2436813 ]\n",
      " [0.22865082 0.22865082 0.1875491  0.18690956 0.08411594 0.08412371]\n",
      " [0.10597074 0.10597105 0.28805834 0.10597074 0.28805834 0.10597076]\n",
      " [0.2147364  0.07899711 0.07899711 0.21472673 0.19780627 0.2147364 ]\n",
      " [0.21115868 0.21115865 0.21115868 0.21115868 0.07768437 0.07768094]\n",
      " [0.18580204 0.07098976 0.18580204 0.18580204 0.18580204 0.18580204]\n",
      " [0.1862933  0.1862933  0.1862933  0.18629329 0.1862933  0.06853348]\n",
      " [0.1862906  0.18629391 0.18629391 0.06853379 0.18629391 0.18629383]\n",
      " [0.18652682 0.1864951  0.18652682 0.18647921 0.06861939 0.18535265]\n",
      " [0.08968846 0.09049137 0.2437985  0.2437985  0.24253471 0.08968846]\n",
      " [0.07768121 0.21115941 0.21115941 0.07768121 0.21115941 0.21115941]\n",
      " [0.14087234 0.12787908 0.3476114  0.12787908 0.12787908 0.12787908]\n",
      " [0.21115588 0.07767991 0.07769661 0.21115588 0.21115588 0.21115588]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666666 0.16666667 0.16666667]\n",
      " [0.18629321 0.06853382 0.18629424 0.18629424 0.18629017 0.18629424]\n",
      " [0.21115941 0.21115941 0.21115941 0.07768121 0.21115941 0.07768121]\n",
      " [0.06853348 0.1862933  0.1862933  0.1862933  0.1862933  0.18629329]\n",
      " [0.24321388 0.09141155 0.24321388 0.08947339 0.24321388 0.08947339]\n",
      " [0.10596307 0.10596884 0.28803658 0.28803748 0.10603093 0.10596307]\n",
      " [0.18634139 0.06855117 0.18634139 0.18634139 0.18634139 0.18608324]\n",
      " [0.19538878 0.19526379 0.07551574 0.1845058  0.19537015 0.15395577]\n",
      " [0.25278103 0.09299295 0.09299295 0.2154591  0.25278103 0.09299295]\n",
      " [0.20777814 0.20777817 0.20777817 0.1952613  0.10496178 0.07644244]\n",
      " [0.2113524  0.0777522  0.2113524  0.2104384  0.2113524  0.0777522 ]\n",
      " [0.16666666 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.20632574 0.09879407 0.20632574 0.20632571 0.20632574 0.075903  ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.08964714 0.2436862  0.2436862  0.08964714 0.2436862  0.08964714]\n",
      " [0.1862933  0.1862933  0.1862933  0.06853348 0.1862933  0.1862933 ]\n",
      " [0.08686176 0.23153956 0.23611474 0.09634581 0.11302343 0.23611474]\n",
      " [0.21117496 0.07768693 0.21117496 0.21117496 0.07768702 0.21110117]\n",
      " [0.10598551 0.10596848 0.10596848 0.2880522  0.10597319 0.2880522 ]\n",
      " [0.21037316 0.08111543 0.21037316 0.21037316 0.07739197 0.21037316]\n",
      " [0.07855043 0.20227271 0.21349153 0.07864097 0.2135222  0.2135222 ]\n",
      " [0.110424   0.28662363 0.28662363 0.10544294 0.10544294 0.10544294]\n",
      " [0.0777671  0.2113841  0.2113929  0.07792644 0.21013653 0.2113929 ]\n",
      " [0.21110094 0.21109837 0.21110094 0.21110094 0.0776597  0.07793915]\n",
      " [0.06859044 0.18644814 0.18644814 0.18644764 0.18561749 0.18644814]\n",
      " [0.2121131  0.07803205 0.07803205 0.2121131  0.20759657 0.2121131 ]\n",
      " [0.10630584 0.10630584 0.28580737 0.10630584 0.28896922 0.10630584]\n",
      " [0.09018597 0.0895941  0.24354202 0.24354184 0.24354202 0.0895941 ]\n",
      " [0.18629333 0.18629333 0.18629333 0.06853349 0.18629318 0.18629333]\n",
      " [0.08964884 0.08964884 0.24369079 0.24367197 0.24369079 0.08964884]\n",
      " [0.18637414 0.18594013 0.18637414 0.18637414 0.18637414 0.06856322]\n",
      " [0.2436862  0.08964714 0.2436862  0.2436862  0.08964714 0.08964714]\n",
      " [0.2436859  0.24368611 0.08964765 0.24368611 0.08964711 0.08964711]\n",
      " [0.07768121 0.21115941 0.21115941 0.07768122 0.21115941 0.21115941]\n",
      " [0.19171624 0.19171642 0.19171683 0.19171683 0.07052869 0.16260494]\n",
      " [0.16668412 0.16668412 0.16668412 0.16668412 0.16668412 0.16657941]\n",
      " [0.13708985 0.23129651 0.23129651 0.08511278 0.23011523 0.08508924]\n",
      " [0.10597087 0.10597087 0.2880587  0.28805786 0.10597087 0.10597087]\n",
      " [0.18622813 0.18630564 0.18631484 0.1862952  0.18631484 0.0685414 ]\n",
      " [0.09309332 0.09309332 0.25305387 0.20894882 0.25305387 0.09875683]\n",
      " [0.13117447 0.08555783 0.2325703  0.2325692  0.2325703  0.08555786]\n",
      " [0.07769103 0.21106042 0.2111859  0.2111859  0.07769095 0.21118578]\n",
      " [0.21125412 0.21080518 0.21125412 0.07771642 0.21125412 0.07771605]\n",
      " [0.24270643 0.08928671 0.08932708 0.09326698 0.24270643 0.24270643]\n",
      " [0.18629804 0.18629804 0.18629804 0.18629804 0.06854191 0.18626595]\n",
      " [0.28805843 0.10597078 0.28805843 0.10597079 0.10597078 0.10597078]\n",
      " [0.08957725 0.09035818 0.24349621 0.08957725 0.24349618 0.24349493]\n",
      " [0.20313843 0.11271585 0.20313843 0.20313843 0.07473046 0.20313843]\n",
      " [0.21115941 0.21115941 0.21115941 0.07768121 0.07768121 0.21115941]\n",
      " [0.21115941 0.21115941 0.21115941 0.21115941 0.07768121 0.07768121]\n",
      " [0.10597083 0.10597078 0.10597078 0.28805843 0.10597078 0.28805843]\n",
      " [0.2112957  0.07775845 0.21064112 0.21136938 0.21117668 0.07775877]]\n",
      "INFO:tensorflow:loss = 1.8891098, step = 1\n",
      "INFO:tensorflow:Loss for final step: 1.8891098.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f4e641c6610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tf.estimator.Estimator(model_fn=conv_model, model_dir=\"convnet_model\")\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_train},\n",
    "    y=y_train,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "clf.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-14-16:10:19\n",
      "INFO:tensorflow:Restoring parameters from convnet_model/model.ckpt-1\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-14-16:10:20\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.17222223, global_step = 1, loss = 1.8654065\n",
      "{'loss': 1.8654065, 'global_step': 1, 'accuracy': 0.17222223}\n"
     ]
    }
   ],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = clf.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "print eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
