{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    min_l, max_l = min(labels), max(labels)\n",
    "    one_hot_map = {k: [0 if i < k else 1 if i == k else 0 \\\n",
    "                              for i in range(min_l, max_l+1)] \\\n",
    "                      for k in range(min_l, max_l+1)}\n",
    "    \n",
    "    return np.array([one_hot_map[label] for label in labels]), len(set([l for l in labels]))\n",
    "\n",
    "def init_weights(data_shape, \n",
    "                 n_channels, \n",
    "                 filter_size_conv1,\n",
    "                 stride_conv1,\n",
    "                 pad_conv1,\n",
    "                 n_filters_conv1,\n",
    "                 filter_size_pool1,\n",
    "                 stride_pool1,\n",
    "                 filter_size_conv2,\n",
    "                 stride_conv2,\n",
    "                 pad_conv2,\n",
    "                 n_filters_conv2,\n",
    "                 filter_size_pool2,\n",
    "                 stride_pool2,\n",
    "                 n_fc_neurons_1, \n",
    "                 n_fc_neurons_2):\n",
    "    \n",
    "    params = {\n",
    "        'conv1': {\n",
    "            'weights': (),  # (W, b)\n",
    "            'h_params': (stride_conv1, pad_conv1),\n",
    "            'activations': ()\n",
    "        },\n",
    "        'pool1': {\n",
    "            'h_params': (stride_pool1, filter_size_pool1),\n",
    "            'activations': ()\n",
    "        },          \n",
    "        'conv2': {\n",
    "            'weights': (),  # (W, b)\n",
    "            'h_params': (stride_conv2, pad_conv2),\n",
    "            'activations': ()\n",
    "        },\n",
    "        'pool2': {\n",
    "            'h_params': (stride_pool2, filter_size_pool2),     \n",
    "            'activations': ()\n",
    "        },\n",
    "        'fc1': {\n",
    "            'weights': (),  # (W, b)\n",
    "            'activations': ()\n",
    "        },\n",
    "        'fc2': {\n",
    "            'weights': (),  # (W, b)\n",
    "            'activations': ()\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Weights for the first convolutional layer\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(filter_size_conv1, filter_size_conv1, n_channels, n_filters_conv1)\n",
    "    np.random.seed(42)\n",
    "    b = np.random.randn(1, 1, 1, n_filters_conv1)\n",
    "    params['conv1']['weights'] = (W, b)\n",
    "\n",
    "    # Weights for the second convolutional layer\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(filter_size_conv2, filter_size_conv2, n_filters_conv1, n_filters_conv2)\n",
    "    np.random.seed(42)\n",
    "    b = np.random.randn(1, 1, 1, n_filters_conv2)\n",
    "    params['conv2']['weights'] = (W, b)    \n",
    "    \n",
    "    # Weights for the first fully connected layer\n",
    "    output_1_shape = int((data_shape[1] + 2 * pad_conv1 - filter_size_conv1)/stride_conv1 + 1)\n",
    "    output_2_shape = int((output_1_shape - filter_size_pool1)/stride_pool1 + 1)\n",
    "    output_3_shape = int((output_2_shape + 2 * pad_conv2 - filter_size_conv2)/stride_conv2 + 1)\n",
    "    output_4_shape = int((output_3_shape - filter_size_pool2)/stride_pool2 + 1)\n",
    "    flattened_dim = output_4_shape**2 * n_filters_conv2\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(flattened_dim, n_fc_neurons_1)\n",
    "    np.random.seed(42)\n",
    "    b = np.random.randn(1, n_fc_neurons_1)\n",
    "    params['fc1']['weights'] = (W, b)\n",
    "    \n",
    "    # Weights for the second fully connected layer    \n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(n_fc_neurons_1, n_fc_neurons_2)\n",
    "    np.random.seed(42)\n",
    "    b = np.random.randn(1, n_fc_neurons_2)    \n",
    "    params['fc2']['weights'] = (W, b)\n",
    "    \n",
    "    return params\n",
    "    \n",
    "def conv_forward(A_prev, W, b, stride=1, pad=1):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    n_H = int((n_H_prev + 2 * pad - f) / stride + 1)\n",
    "    n_W = int((n_W_prev + 2 * pad - f) / stride + 1)    \n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    A_prev_pad = np.pad(A_prev, \n",
    "                        ((0,0), (pad,pad), (pad,pad), (0,0)), \n",
    "                        mode='constant', \n",
    "                        constant_values=(0))\n",
    "    cache = np.zeros((m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    Z[i, h, w, c] = np.sum(np.multiply(a_slice_prev, W[:,:,:,c]) + b[:,:,:,c])\n",
    "                    \n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def conv_backward():\n",
    "    pass\n",
    "\n",
    "def pool_forward(A_prev, stride=2, filter_size=2):\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    n_H = int((n_H_prev - filter_size) / stride + 1)\n",
    "    n_W = int((n_W_prev - filter_size) / stride + 1)  \n",
    "\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C)) \n",
    "    cache = np.zeros((m))\n",
    "    \n",
    "    for i in range(m):                          \n",
    "        for h in range(n_H):                     \n",
    "            for w in range(n_W):                 \n",
    "                for c in range (n_C):            \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + filter_size\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + filter_size\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    cache[i] = np.argmax(a_prev_slice)\n",
    "                \n",
    "    return A, cache\n",
    "\n",
    "def pool_backward():\n",
    "    pass\n",
    "\n",
    "def relu_activation_forward(A_prev):\n",
    "    return np.where(A_prev>0, A_prev, 0)\n",
    "\n",
    "def fc_forward(A_prev, W, b):\n",
    "    A_prev = A_prev.reshape(A_prev.shape[0], -1)    \n",
    "    A = np.matmul(A_prev, W) + b\n",
    "    \n",
    "    return A \n",
    "\n",
    "def fc_backward(m, labels, activations, W):\n",
    "    (a1, a2, a3) = activations\n",
    "    _m= 1/m\n",
    "\n",
    "    dL_dW2 = _m * np.dot(a2, (a3-labels)).T\n",
    "    dL_db2 = _m * np.sum((a3-labels), axis=0, keepdims=True).T\n",
    "    dL_dW1 = _m * np.dot(np.multiply(np.dot((a3-labels), W).T, (a2 * (1-a2))), a1)\n",
    "    dL_db1 = _m * np.sum(np.multiply(np.dot((a3-labels), W).T, (a2 * (1-a2))), axis=1, keepdims=True)   \n",
    "        \n",
    "    return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
    "\n",
    "def sigmoid_activation_forward(z):\n",
    "    z = z.clip(min=-500, max=500)\n",
    "    exp = np.exp(z)\n",
    "    \n",
    "    return exp/(exp+1)\n",
    "\n",
    "def softmax_activation_forward(z):\n",
    "    z = z.clip(min=-500, max=500)\n",
    "    exp = np.exp(z)\n",
    "    \n",
    "    return exp/exp.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "def log_loss(output, labels): \n",
    "    output = output.clip(min=1e-10, max=0.9999999999)   \n",
    "    cost = (-1./labels.shape[0]) * np.sum(\n",
    "        np.multiply(labels, np.log(output)) + \n",
    "        np.multiply((1-labels), np.log(1-output)))\n",
    " \n",
    "    return cost\n",
    "\n",
    "def forward(X, params, inspect=False):\n",
    "    if inspect:\n",
    "        print('X.shape: {}\\n'.format(X.shape))\n",
    "    \n",
    "    # First convolutional layer\n",
    "    W, b = params['conv1']['weights']\n",
    "    stride, pad = params['conv1']['h_params']\n",
    "    Z, cache_conv1 = conv_forward(X, W, b, stride=stride, pad=pad)\n",
    "    if inspect:\n",
    "        print('First convolutional layer:\\nW.shape: {}\\nb.shape: {}\\nZ.shape: {}\\n'.format(\n",
    "            W.shape, b.shape, Z.shape))\n",
    "    \n",
    "    # First max pool layer and ReLU activation\n",
    "    stride, filter_size = params['pool1']['h_params']\n",
    "    Z, cache_pool1 = pool_forward(Z, stride=stride, filter_size=filter_size)\n",
    "    A = relu_activation_forward(Z)\n",
    "    params['pool1']['activations'] = (cache_pool1, A)\n",
    "    if inspect:\n",
    "        print('First max pooling layer:\\nA.shape: {}\\n'.format(\n",
    "            A.shape))\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    W, b = params['conv2']['weights']\n",
    "    stride, pad = params['conv2']['h_params']\n",
    "    Z, cache_conv2 = conv_forward(A, W, b, stride=stride, pad=pad)\n",
    "    if inspect:\n",
    "        print('Second convolutional layer:\\nW.shape: {}\\nb.shape: {}\\nZ.shape: {}\\n'.format(\n",
    "            W.shape, b.shape, Z.shape))\n",
    "    \n",
    "    # Second max pool layer and ReLU activation\n",
    "    stride, filter_size = params['pool2']['h_params']\n",
    "    Z, cache_pool2 = pool_forward(Z, stride=stride, filter_size=filter_size)\n",
    "    A = relu_activation_forward(Z)\n",
    "    params['pool2']['activations'] = (cache_pool2, A)\n",
    "    params['fc2']['activations'] = [A]\n",
    "    if inspect:\n",
    "        print('Second max pooling layer:\\nA.shape: {}\\n'.format(\n",
    "            A.shape))\n",
    "    \n",
    "    # First fully connnected layer and ReLU activation\n",
    "    W, b = params['fc1']['weights']\n",
    "    Z = fc_forward(A, W, b)\n",
    "    A = relu_activation_forward(Z)\n",
    "    params['fc2']['activations'].append(A)\n",
    "    if inspect:\n",
    "        print('First fully connected layer:\\nW.shape: {}\\nb.shape: {}\\nZ.shape: {}\\nA.shape: {}\\n'.format(\n",
    "            W.shape, b.shape, Z.shape, A.shape))\n",
    "    \n",
    "    # Second fully connected layer and sigmoid activation\n",
    "    W, b = params['fc2']['weights']\n",
    "    Z = fc_forward(A, W, b)\n",
    "    A = sigmoid_activation_forward(Z)\n",
    "    params['fc2']['activations'].append(A)\n",
    "    if inspect:\n",
    "        print('Second fully connected layer:\\nW.shape: {}\\nb.shape: {}\\nZ.shape: {}\\nA.shape: {}\\n'.format(\n",
    "            W.shape, b.shape, Z.shape, A.shape))\n",
    "    \n",
    "    return A\n",
    "\n",
    "def predict(z):    \n",
    "    return softmax_activation_forward(z)\n",
    "\n",
    "def backward(m, labels, params):\n",
    "    \"\"\"\n",
    "    dL_dW_fc1, dL_db_fc1, dL_dW_fc2, dL_db_fc2 = fc_backward(\n",
    "            m, labels, params['fc2']['activations'], params['fc2']['weights'])\n",
    "    dL_relu2 = relu_backward()\n",
    "    dL_pool2 = pool_backward()\n",
    "    dL_conv2 = conv_backward()\n",
    "    dL_relu1 = relu_backward()\n",
    "    dL_pool1 = pool_backward()\n",
    "    dL_conv1 = conv_backward()\n",
    "    \n",
    "    derivatives = {\n",
    "        'conv1': dL_conv1,\n",
    "        'pool1': dL_pool1,\n",
    "        'relu1': dL_relu1,\n",
    "        'conv2': dL_conv2,\n",
    "        'pool2': dL_pool2,\n",
    "        'relu2': dL_relu2,\n",
    "        'fc1': (dL_dW_fc1, dL_db_fc1),\n",
    "        'fc2': (dL_dW_fc2, dL_db_fc2)\n",
    "    }\n",
    "\n",
    "    return derivatives\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def train_network(X, labels, n_classes, iters, inspect=False):\n",
    "    m = X.shape[0]\n",
    "    params = init_weights(data_shape=X.shape, \n",
    "                          n_channels=3, \n",
    "                          filter_size_conv1=3, \n",
    "                          stride_conv1=1,\n",
    "                          pad_conv1=1,\n",
    "                          n_filters_conv1=8,\n",
    "                          filter_size_pool1=2,\n",
    "                          stride_pool1=2,\n",
    "                          filter_size_conv2=3,\n",
    "                          stride_conv2=1,\n",
    "                          pad_conv2=1,\n",
    "                          n_filters_conv2=8,\n",
    "                          filter_size_pool2=2,\n",
    "                          stride_pool2=2,\n",
    "                          n_fc_neurons_1=50,\n",
    "                          n_fc_neurons_2=n_classes)\n",
    "    \n",
    "    initial_pass = forward(X, params, inspect=inspect)\n",
    "    initial_loss = log_loss(initial_pass, labels)\n",
    "    print('Initial log loss is {}\\n'.format(initial_loss))\n",
    "    \n",
    "    for i in range(iters):\n",
    "        output = forward(X, params)\n",
    "        loss = log_loss(output, labels)\n",
    "        print('Iteration {} -- log loss: {}'.format(i+1, loss))\n",
    "        derivatives = backward(m, labels, params)\n",
    "    \n",
    "    final_pass = forward(X, params)\n",
    "    final_loss = log_loss(final_pass, labels)\n",
    "    print('\\nFinal log loss is {}\\n'.format(final_loss))\n",
    "    \n",
    "    predictions = predict(final_pass)\n",
    "    print('Predictions:\\n{}'.format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('ex5_train_x.npy')\n",
    "y = np.load('ex5_train_y.npy')\n",
    "labels, n_classes = one_hot_encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (5, 64, 64, 3)\n",
      "\n",
      "First convolutional layer:\n",
      "W.shape: (3, 3, 3, 8)\n",
      "b.shape: (1, 1, 1, 8)\n",
      "Z.shape: (5, 64, 64, 8)\n",
      "\n",
      "First max pooling layer:\n",
      "A.shape: (5, 32, 32, 8)\n",
      "\n",
      "Second convolutional layer:\n",
      "W.shape: (3, 3, 8, 8)\n",
      "b.shape: (1, 1, 1, 8)\n",
      "Z.shape: (5, 32, 32, 8)\n",
      "\n",
      "Second max pooling layer:\n",
      "A.shape: (5, 16, 16, 8)\n",
      "\n",
      "First fully connected layer:\n",
      "W.shape: (2048, 50)\n",
      "b.shape: (1, 50)\n",
      "Z.shape: (5, 50)\n",
      "A.shape: (5, 50)\n",
      "\n",
      "Second fully connected layer:\n",
      "W.shape: (50, 6)\n",
      "b.shape: (1, 6)\n",
      "Z.shape: (5, 6)\n",
      "A.shape: (5, 6)\n",
      "\n",
      "Initial log loss is 69.0775525915\n",
      "\n",
      "Iteration 1 -- log loss: 69.0775525915\n",
      "Iteration 2 -- log loss: 69.0775525915\n",
      "Iteration 3 -- log loss: 69.0775525915\n",
      "\n",
      "Final log loss is 69.0775525915\n",
      "\n",
      "Predictions:\n",
      "[[0.28805844 0.10597078 0.10597078 0.10597078 0.10597078 0.28805844]\n",
      " [0.24368619 0.08964714 0.24368619 0.08964714 0.08964714 0.24368619]\n",
      " [0.28805844 0.10597078 0.10597078 0.10597078 0.10597078 0.28805844]\n",
      " [0.2111594  0.0776812  0.2111594  0.0776812  0.2111594  0.2111594 ]\n",
      " [0.24368619 0.08964714 0.08964714 0.08964714 0.24368619 0.24368619]]\n"
     ]
    }
   ],
   "source": [
    "train_network(X[:5], labels[:5], n_classes, iters=3, inspect=True)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
